#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºæ•´åˆèƒ½åŠ›çš„æ•°æ®é›†æ„å»ºå™¨ (æ”¯æŒAlpaca/ShareGPTæ ¼å¼)
ä¸“é—¨æ„å»ºéœ€è¦ä¿¡æ¯æ•´åˆã€é•¿ä¸Šä¸‹æ–‡ç†è§£å’Œå¤æ‚æ¨ç†çš„è®­ç»ƒæ•°æ®
å®Œå…¨å…¼å®¹Alpacaå’ŒShareGPTæ ¼å¼æ ‡å‡†
"""

import json
import random
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, field
import re
from datetime import datetime
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class IntegrationTask:
    """ä¿¡æ¯æ•´åˆä»»åŠ¡"""
    task_type: str  # ä»»åŠ¡ç±»å‹ï¼šcomparison, synthesis, analysis, reasoning
    instruction: str  # æŒ‡ä»¤
    context: str  # é•¿ä¸Šä¸‹æ–‡ï¼ˆå¤šä¸ªç›¸å…³æ–‡æ¡£å—ï¼‰
    expected_output: str  # æœŸæœ›çš„æ•´åˆç­”æ¡ˆ
    difficulty: str  # éš¾åº¦çº§åˆ«ï¼šbasic, intermediate, advanced
    integration_points: List[str] = field(default_factory=list)  # éœ€è¦æ•´åˆçš„å…³é”®ç‚¹
    quality_score: float = 0.0  # è´¨é‡è¯„åˆ†
    context_sources: List[str] = field(default_factory=list)  # ä¸Šä¸‹æ–‡æ¥æº

class ConfigurableIntegrationDatasetBuilder:
    """å¯é…ç½®çš„å¢å¼ºæ•´åˆæ•°æ®é›†æ„å»ºå™¨ï¼Œæ”¯æŒAlpaca/ShareGPTæ ¼å¼"""
    
    def __init__(self, input_dir: str = "output", output_dir: str = "output/integration_dataset", 
                 config_file: str = "integration_config.json"):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½é…ç½®
        self.config = self._load_config(config_file)
        
        # åŠ è½½æ•°æ®
        self.documents_chunks = self._load_json("vectorization/documents_chunks.json")
        self.sections_chunks = self._load_json("vectorization/sections_chunks.json")
        self.analysis_result = self._load_json("analysis/analysis_result.json")
        
        # åŠ è½½åµŒå…¥å‘é‡ç”¨äºè¯­ä¹‰ç›¸ä¼¼æ€§è®¡ç®—
        self.embeddings = self._load_embeddings()
        
        logger.info(f"åŠ è½½äº† {len(self.documents_chunks)} ä¸ªæ–‡æ¡£å—")
        logger.info(f"é…ç½®æ–‡ä»¶: {config_file}")
        
    def _load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_path = Path(config_file)
        if not config_path.exists():
            logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
                return config
        except Exception as e:
            logger.error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            "dataset_config": {
                "max_samples": 1000,
                "task_distribution": {
                    "comparison": 0.25,
                    "synthesis": 0.30,
                    "analysis": 0.25,
                    "reasoning": 0.20
                },
                "difficulty_distribution": {
                    "basic": 0.20,
                    "intermediate": 0.50,
                    "advanced": 0.30
                }
            },
            "integration_requirements": {
                "min_context_length": 500,
                "max_context_length": 8000,
                "min_chunks_per_task": 2,
                "max_chunks_per_task": 6,
                "similarity_threshold": 0.6
            },
            "output_format": {
                "alpaca_format": True,
                "sharegpt_format": False,
                "include_metadata": True,
                "metadata_fields": [
                    "task_type",
                    "difficulty", 
                    "integration_points",
                    "context_sources",
                    "quality_score"
                ]
            }
        }
    
    def _load_json(self, relative_path: str) -> List[Dict]:
        """åŠ è½½JSONæ–‡ä»¶"""
        file_path = self.input_dir / relative_path
        if not file_path.exists():
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return []
    
    def _load_embeddings(self) -> Optional[np.ndarray]:
        """åŠ è½½æ–‡æ¡£åµŒå…¥å‘é‡"""
        embeddings_path = self.input_dir / "enhanced_embeddings/documents_embeddings.npy"
        if embeddings_path.exists():
            try:
                return np.load(embeddings_path)
            except Exception as e:
                logger.warning(f"åŠ è½½åµŒå…¥å‘é‡å¤±è´¥: {e}")
        return None
    
    def find_related_chunks(self, base_chunk: Dict, max_related: int = 5, similarity_threshold: float = 0.5) -> List[Dict]:
        """æ‰¾åˆ°ä¸åŸºç¡€å—ç›¸å…³çš„å…¶ä»–æ–‡æ¡£å—"""
        if self.embeddings is None or len(self.embeddings) != len(self.documents_chunks):
            return self._find_related_chunks_by_keywords(base_chunk, max_related)
        
        base_idx = next((i for i, chunk in enumerate(self.documents_chunks) if chunk == base_chunk), None)
        if base_idx is None:
            return []
        
        base_embedding = self.embeddings[base_idx]
        similarities = []
        
        for i, embedding in enumerate(self.embeddings):
            if i != base_idx:
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = np.dot(base_embedding, embedding) / (
                    np.linalg.norm(base_embedding) * np.linalg.norm(embedding)
                )
                similarities.append((i, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶é€‰æ‹©æœ€ç›¸å…³çš„
        similarities.sort(key=lambda x: x[1], reverse=True)
        related_chunks = []
        
        for idx, sim in similarities[:max_related]:
            if sim >= similarity_threshold:
                related_chunks.append(self.documents_chunks[idx])
        
        return related_chunks
    
    def _find_related_chunks_by_keywords(self, base_chunk: Dict, max_related: int = 5) -> List[Dict]:
        """åŸºäºå…³é”®è¯æ‰¾åˆ°ç›¸å…³æ–‡æ¡£å—"""
        base_text = base_chunk.get("text", "")
        base_metadata = base_chunk.get("metadata", {})
        
        # æå–å…³é”®æŠ€æœ¯è¯æ±‡
        keywords = self._extract_technical_keywords(base_text)
        chip_category = base_metadata.get("chip_category", "")
        
        related_chunks = []
        scores = []
        
        for chunk in self.documents_chunks:
            if chunk == base_chunk:
                continue
                
            chunk_text = chunk.get("text", "")
            chunk_metadata = chunk.get("metadata", {})
            
            score = 0
            
            # ç›¸åŒèŠ¯ç‰‡ç±»åˆ«åŠ åˆ†
            if chip_category and chunk_metadata.get("chip_category") == chip_category:
                score += 2
            
            # å…³é”®è¯åŒ¹é…åŠ åˆ†
            for keyword in keywords:
                if keyword in chunk_text:
                    score += 1
            
            # æ–‡æ¡£ç±»å‹ç›¸å…³æ€§åŠ åˆ†
            if self._is_complementary_doc_type(
                base_metadata.get("document_type", ""),
                chunk_metadata.get("document_type", "")
            ):
                score += 1
            
            if score > 0:
                scores.append((chunk, score))
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›æœ€ç›¸å…³çš„
        scores.sort(key=lambda x: x[1], reverse=True)
        return [chunk for chunk, _ in scores[:max_related]]
    
    def _extract_technical_keywords(self, text: str) -> List[str]:
        """æå–æŠ€æœ¯å…³é”®è¯"""
        # æŠ€æœ¯æœ¯è¯­è¯å…¸
        technical_terms = [
            "PWM", "ADC", "ä¸­æ–­", "å¯„å­˜å™¨", "å¼•è„š", "ç®¡è„š", "æ—¶é’Ÿ", "åˆ†é¢‘",
            "é‡‡æ ·", "è½¬æ¢", "é…ç½®", "ä½¿èƒ½", "ç¦ç”¨", "æ¨¡å¼", "åŠŸè€—", "ç”µæµ",
            "ç”µå‹", "é¢‘ç‡", "ç²¾åº¦", "åˆ†è¾¨ç‡", "é€šé“", "ç¼“å†²", "æ»¤æ³¢",
            "è§¦å‘", "åŒæ­¥", "å¼‚æ­¥", "ä¸²è¡Œ", "å¹¶è¡Œ", "é€šä¿¡", "åè®®"
        ]
        
        keywords = []
        text_upper = text.upper()
        
        for term in technical_terms:
            if term in text or term.upper() in text_upper:
                keywords.append(term)
        
        # æå–è‹±æ–‡ç¼©å†™
        acronyms = re.findall(r'\b[A-Z]{2,}\b', text)
        keywords.extend(acronyms)
        
        return list(set(keywords))
    
    def _is_complementary_doc_type(self, type1: str, type2: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªæ–‡æ¡£ç±»å‹æ˜¯å¦äº’è¡¥"""
        complementary_pairs = [
            ("datasheet", "user manual"),
            ("datasheet", "application"),
            ("troubleshooting", "user manual"),
            ("selection", "datasheet"),
            ("brochure", "datasheet")
        ]
        
        type1_lower = type1.lower()
        type2_lower = type2.lower()
        
        for pair in complementary_pairs:
            if (pair[0] in type1_lower and pair[1] in type2_lower) or \
               (pair[1] in type1_lower and pair[0] in type2_lower):
                return True
        
        return False
    
    def build_integration_dataset(self) -> List[IntegrationTask]:
        """æ„å»ºä¿¡æ¯æ•´åˆæ•°æ®é›†"""
        logger.info("å¼€å§‹æ„å»ºä¿¡æ¯æ•´åˆæ•°æ®é›†...")
        
        config = self.config["dataset_config"]
        requirements = self.config["integration_requirements"]
        
        max_samples = config["max_samples"]
        task_distribution = config["task_distribution"]
        
        dataset = []
        task_counts = {task_type: 0 for task_type in task_distribution.keys()}
        
        # è®¡ç®—æ¯ç§ä»»åŠ¡ç±»å‹çš„ç›®æ ‡æ•°é‡
        target_counts = {}
        for task_type, ratio in task_distribution.items():
            target_counts[task_type] = int(max_samples * ratio)
        
        # éšæœºå¤„ç†æ–‡æ¡£å—ä»¥å¢åŠ å¤šæ ·æ€§
        shuffled_chunks = self.documents_chunks.copy()
        random.shuffle(shuffled_chunks)
        
        for i, base_chunk in enumerate(shuffled_chunks):
            if len(dataset) >= max_samples:
                break
            
            # è·³è¿‡å†…å®¹è¿‡çŸ­çš„å—
            if len(base_chunk.get("text", "")) < requirements["min_context_length"]:
                continue
            
            # æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å—
            similarity_threshold = requirements["similarity_threshold"]
            max_related = requirements["max_chunks_per_task"] - 1
            related_chunks = self.find_related_chunks(base_chunk, max_related, similarity_threshold)
            
            if len(related_chunks) < requirements["min_chunks_per_task"] - 1:
                continue
            
            # æ„å»ºchunkç»„åˆ
            chunk_group = [base_chunk] + related_chunks
            
            # æ£€æŸ¥ä¸Šä¸‹æ–‡é•¿åº¦
            total_context = self._build_context_text(chunk_group)
            if len(total_context) > requirements["max_context_length"]:
                # å‡å°‘chunkæ•°é‡
                chunk_group = chunk_group[:requirements["min_chunks_per_task"]]
                total_context = self._build_context_text(chunk_group)
            
            if len(total_context) < requirements["min_context_length"]:
                continue
            
            # æ™ºèƒ½åˆ†æé€‚åˆçš„ä»»åŠ¡ç±»å‹
            suitable_task_types = self._analyze_content_suitability(chunk_group)
            
            # å¦‚æœæ²¡æœ‰åˆé€‚çš„ä»»åŠ¡ç±»å‹ï¼Œè·³è¿‡
            if not suitable_task_types:
                continue
            
            # æŒ‰éœ€æ±‚å’Œä¼˜å…ˆçº§åˆ›å»ºä»»åŠ¡
            for task_type in suitable_task_types:
                if task_counts.get(task_type, 0) >= target_counts.get(task_type, 0):
                    continue
                
                task = self._create_task_by_type(task_type, chunk_group)
                
                if task and self._validate_task_quality(task):
                    dataset.append(task)
                    task_counts[task_type] += 1
                    logger.debug(f"åˆ›å»ºäº† {task_type} ä»»åŠ¡ï¼ŒåŸºäºå†…å®¹: {base_chunk.get('metadata', {}).get('chip_category', 'Unknown')}")
                    break  # æ¯ä¸ªchunkç»„åˆåªåˆ›å»ºä¸€ä¸ªä»»åŠ¡
        
        logger.info(f"ç”Ÿæˆäº† {len(dataset)} ä¸ªæ•´åˆä»»åŠ¡")
        logger.info(f"ä»»åŠ¡åˆ†å¸ƒ: {task_counts}")
        
        return dataset
    
    def _build_context_text(self, chunks: List[Dict]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡æ–‡æœ¬"""
        context_parts = []
        for chunk in chunks:
            metadata = chunk.get("metadata", {})
            chip_category = metadata.get("chip_category", "æŠ€æœ¯èµ„æ–™")
            text = chunk.get("text", "")
            context_parts.append(f"=== {chip_category} ===\n{text}\n")
        return "\n".join(context_parts)
    
    def _analyze_content_suitability(self, chunks: List[Dict]) -> List[str]:
        """åˆ†æå†…å®¹é€‚åˆæ€§ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        suitable_tasks = []
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬ç”¨äºåˆ†æ
        combined_text = " ".join([chunk.get("text", "") for chunk in chunks])
        combined_metadata = [chunk.get("metadata", {}) for chunk in chunks]
        
        # è·å–æ‰€æœ‰èŠ¯ç‰‡ç±»åˆ«å’Œæ–‡æ¡£ç±»å‹
        chip_categories = [meta.get("chip_category", "") for meta in combined_metadata]
        doc_types = [meta.get("document_type", "") for meta in combined_metadata]
        
        # ç®€å•çš„å¯å‘å¼è§„åˆ™
        unique_chips = set([cat for cat in chip_categories if cat])
        
        # æ¯”è¾ƒåˆ†æï¼šéœ€è¦å¤šä¸ªä¸åŒçš„èŠ¯ç‰‡
        if len(unique_chips) >= 2:
            suitable_tasks.append("comparison")
        
        # ç»¼åˆè®¾è®¡ï¼šéœ€è¦å¤šä¸ªæ¨¡å—
        if len(chunks) >= 3:
            suitable_tasks.append("synthesis")
        
        # æ·±åº¦åˆ†æï¼šéœ€è¦æŠ€æœ¯æ·±åº¦å†…å®¹
        tech_indicators = ["åŸç†", "æœºåˆ¶", "æ¶æ„", "å®ç°", "å¯„å­˜å™¨"]
        if sum(1 for indicator in tech_indicators if indicator in combined_text) >= 2:
            suitable_tasks.append("analysis")
        
        # æ¨ç†è§£å†³ï¼šæœ‰é—®é¢˜è§£å†³å†…å®¹
        problem_indicators = ["æ•…éšœ", "é—®é¢˜", "ä¼˜åŒ–", "è¯Šæ–­"]
        if sum(1 for indicator in problem_indicators if indicator in combined_text) >= 1:
            suitable_tasks.append("reasoning")
        
        return suitable_tasks
    
    def _create_task_by_type(self, task_type: str, chunks: List[Dict]) -> Optional[IntegrationTask]:
        """æ ¹æ®ç±»å‹åˆ›å»ºä»»åŠ¡"""
        if task_type == "comparison":
            return self._create_comparison_task(chunks)
        elif task_type == "synthesis":
            return self._create_synthesis_task(chunks)
        elif task_type == "analysis":
            return self._create_analysis_task(chunks)
        elif task_type == "reasoning":
            return self._create_reasoning_task(chunks)
        
        return None
    
    def _create_comparison_task(self, chunks: List[Dict]) -> Optional[IntegrationTask]:
        """åˆ›å»ºæ¯”è¾ƒåˆ†æä»»åŠ¡"""
        # è·å–é…ç½®æ¨¡æ¿
        templates = self.config.get("task_templates", {}).get("comparison", {})
        focus_areas = templates.get("focus_areas", ["æŠ€æœ¯ç‰¹æ€§å¯¹æ¯”"])
        
        focus_area = random.choice(focus_areas)
        instruction = f"æ¯”è¾ƒä»¥ä¸‹æŠ€æœ¯èµ„æ–™ä¸­çš„{focus_area}ï¼Œåˆ†æå„æ–¹æ¡ˆçš„ä¼˜åŠ£åŠ¿å’Œé€‚ç”¨åœºæ™¯"
        
        context = self._build_context_text(chunks)
        
        # ç”Ÿæˆå¯¹æ¯”åˆ†æè¾“å‡º
        output = self._generate_comparison_output(chunks, focus_area)
        
        # æå–ä¸Šä¸‹æ–‡æ¥æº
        context_sources = [chunk.get("metadata", {}).get("chip_category", "Unknown") for chunk in chunks]
        
        return IntegrationTask(
            task_type="comparison",
            instruction=instruction,
            context=context,
            expected_output=output,
            difficulty=self._determine_difficulty(chunks),
            integration_points=[
                "æ¯”è¾ƒåˆ†æå¤šä¸ªæŠ€æœ¯æ–¹æ¡ˆ",
                "è¯†åˆ«å…³é”®æŠ€æœ¯å·®å¼‚",
                "è¯„ä¼°ä¼˜åŠ£åŠ¿å’Œé€‚ç”¨æ€§",
                "æä¾›é€‰å‹å»ºè®®"
            ],
            quality_score=self._calculate_quality_score(instruction, context, output),
            context_sources=context_sources
        )
    
    def _create_synthesis_task(self, chunks: List[Dict]) -> Optional[IntegrationTask]:
        """åˆ›å»ºç»¼åˆè®¾è®¡ä»»åŠ¡"""
        templates = self.config.get("task_templates", {}).get("synthesis", {})
        solution_types = templates.get("solution_types", ["ç³»ç»Ÿæ¶æ„è®¾è®¡"])
        
        solution_type = random.choice(solution_types)
        instruction = f"åŸºäºä»¥ä¸‹æŠ€æœ¯èµ„æ–™ï¼Œè®¾è®¡ä¸€ä¸ªå®Œæ•´çš„{solution_type}æ–¹æ¡ˆ"
        
        context = self._build_context_text(chunks)
        output = self._generate_synthesis_output(chunks, solution_type)
        
        context_sources = [chunk.get("metadata", {}).get("chip_category", "Unknown") for chunk in chunks]
        
        return IntegrationTask(
            task_type="synthesis",
            instruction=instruction,
            context=context,
            expected_output=output,
            difficulty=self._determine_difficulty(chunks),
            integration_points=[
                "æ•´åˆå¤šä¸ªæŠ€æœ¯æ¨¡å—",
                "è®¾è®¡ç³»ç»Ÿæ¶æ„",
                "åˆ¶å®šå®ç°ç­–ç•¥",
                "è€ƒè™‘ç³»ç»Ÿä¼˜åŒ–"
            ],
            quality_score=self._calculate_quality_score(instruction, context, output),
            context_sources=context_sources
        )
    
    def _create_analysis_task(self, chunks: List[Dict]) -> Optional[IntegrationTask]:
        """åˆ›å»ºæ·±åº¦åˆ†æä»»åŠ¡"""
        templates = self.config.get("task_templates", {}).get("analysis", {})
        analysis_dimensions = templates.get("analysis_dimensions", ["æŠ€æœ¯åŸç†æ·±åº¦åˆ†æ"])
        
        analysis_dimension = random.choice(analysis_dimensions)
        instruction = f"åŸºäºä»¥ä¸‹æŠ€æœ¯èµ„æ–™ï¼Œè¿›è¡Œ{analysis_dimension}ï¼Œé˜è¿°æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹"
        
        context = self._build_context_text(chunks)
        output = self._generate_analysis_output(chunks, analysis_dimension)
        
        context_sources = [chunk.get("metadata", {}).get("chip_category", "Unknown") for chunk in chunks]
        
        return IntegrationTask(
            task_type="analysis",
            instruction=instruction,
            context=context,
            expected_output=output,
            difficulty=self._determine_difficulty(chunks),
            integration_points=[
                "æ·±åº¦åˆ†ææŠ€æœ¯åŸç†",
                "é˜è¿°å®ç°æœºåˆ¶",
                "æä¾›æŠ€æœ¯æ´å¯Ÿ",
                "ç³»ç»Ÿæ€§çŸ¥è¯†æ•´åˆ"
            ],
            quality_score=self._calculate_quality_score(instruction, context, output),
            context_sources=context_sources
        )
    
    def _create_reasoning_task(self, chunks: List[Dict]) -> Optional[IntegrationTask]:
        """åˆ›å»ºæ¨ç†è§£å†³ä»»åŠ¡"""
        templates = self.config.get("task_templates", {}).get("reasoning", {})
        problem_categories = templates.get("problem_categories", ["æŠ€æœ¯é—®é¢˜è¯Šæ–­"])
        
        problem_category = random.choice(problem_categories)
        instruction = f"åŸºäºä»¥ä¸‹æŠ€æœ¯èµ„æ–™ï¼Œåˆ†æ{problem_category}çš„è§£å†³æ–¹æ¡ˆ"
        
        context = self._build_context_text(chunks)
        output = self._generate_reasoning_output(chunks, problem_category)
        
        context_sources = [chunk.get("metadata", {}).get("chip_category", "Unknown") for chunk in chunks]
        
        return IntegrationTask(
            task_type="reasoning",
            instruction=instruction,
            context=context,
            expected_output=output,
            difficulty=self._determine_difficulty(chunks),
            integration_points=[
                "åˆ†æé—®é¢˜æ ¹å› ",
                "æ¨ç†è§£å†³è·¯å¾„",
                "åˆ¶å®šåº”å¯¹ç­–ç•¥",
                "æä¾›å®æ–½æ–¹æ¡ˆ"
            ],
            quality_score=self._calculate_quality_score(instruction, context, output),
            context_sources=context_sources
        )
    
    def _generate_comparison_output(self, chunks: List[Dict], focus_area: str) -> str:
        """ç”Ÿæˆæ¯”è¾ƒåˆ†æè¾“å‡º"""
        return f"""åŸºäºæä¾›çš„æŠ€æœ¯èµ„æ–™ï¼Œå¯¹{focus_area}è¿›è¡Œæ·±å…¥æ¯”è¾ƒåˆ†æï¼š

## æŠ€æœ¯æ–¹æ¡ˆå¯¹æ¯”

å„æŠ€æœ¯æ–¹æ¡ˆåœ¨{focus_area}æ–¹é¢è¡¨ç°å‡ºä¸åŒçš„ç‰¹ç‚¹å’Œä¼˜åŠ¿ï¼š

### æ ¸å¿ƒå·®å¼‚åˆ†æ
1. **å®ç°æ–¹å¼å·®å¼‚**ï¼šä¸åŒæ–¹æ¡ˆé‡‡ç”¨çš„æŠ€æœ¯è·¯çº¿å’Œå®ç°æ–¹æ³•å­˜åœ¨å·®å¼‚
2. **æ€§èƒ½ç‰¹ç‚¹å¯¹æ¯”**ï¼šåœ¨å…³é”®æ€§èƒ½æŒ‡æ ‡ä¸Šå„æœ‰ä¼˜åŠ£
3. **é€‚ç”¨åœºæ™¯åˆ†æ**ï¼šé’ˆå¯¹ä¸åŒåº”ç”¨åœºæ™¯çš„é€‚é…æ€§ä¸åŒ

## ç»¼åˆè¯„ä¼°ç»“è®º

### ä¼˜åŠ¿å¯¹æ¯”
- å„æ–¹æ¡ˆåœ¨ç‰¹å®šåº”ç”¨åœºæ™¯ä¸‹éƒ½æœ‰å…¶ç‹¬ç‰¹ä¼˜åŠ¿
- éœ€è¦æ ¹æ®å…·ä½“éœ€æ±‚è¿›è¡Œæƒè¡¡é€‰æ‹©

### é€‰å‹å»ºè®®
1. æ ¹æ®å…·ä½“åº”ç”¨éœ€æ±‚ç¡®å®šå…³é”®æ€§èƒ½è¦æ±‚
2. ç»¼åˆè€ƒè™‘æŠ€æœ¯æˆç†Ÿåº¦ã€æˆæœ¬å’Œç»´æŠ¤ä¾¿åˆ©æ€§
3. å»ºè®®è¿›è¡ŒåŸå‹éªŒè¯ä»¥ç¡®è®¤æ–¹æ¡ˆå¯è¡Œæ€§
4. è€ƒè™‘æœªæ¥æŠ€æœ¯æ¼”è¿›å’Œæ‰©å±•éœ€æ±‚"""
    
    def _generate_synthesis_output(self, chunks: List[Dict], solution_type: str) -> str:
        """ç”Ÿæˆç»¼åˆè®¾è®¡è¾“å‡º"""
        return f"""åŸºäºæä¾›çš„æŠ€æœ¯èµ„æ–™ï¼Œè®¾è®¡{solution_type}çš„å®Œæ•´æ–¹æ¡ˆï¼š

## æ–¹æ¡ˆæ¶æ„è®¾è®¡

### ç³»ç»Ÿæ€»ä½“æ¶æ„
åŸºäºæŠ€æœ¯èµ„æ–™åˆ†æï¼Œè®¾è®¡é‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š

1. **æ ¸å¿ƒæ§åˆ¶æ¨¡å—**ï¼šè´Ÿè´£ç³»ç»Ÿä¸»è¦æ§åˆ¶é€»è¾‘
2. **åŠŸèƒ½å¤„ç†æ¨¡å—**ï¼šå®ç°ç‰¹å®šåŠŸèƒ½å¤„ç†
3. **æ¥å£é€‚é…æ¨¡å—**ï¼šå¤„ç†å¤–éƒ¨æ¥å£å’Œé€šä¿¡
4. **ç›‘æ§ç®¡ç†æ¨¡å—**ï¼šç³»ç»ŸçŠ¶æ€ç›‘æ§å’Œç®¡ç†

### å…³é”®æŠ€æœ¯å®ç°

#### æŠ€æœ¯æ–¹æ¡ˆé€‰æ‹©
- ç»¼åˆè€ƒè™‘æ€§èƒ½ã€æˆæœ¬ã€å¯é æ€§ç­‰å› ç´ 
- é‡‡ç”¨æˆç†Ÿç¨³å®šçš„æŠ€æœ¯æ–¹æ¡ˆ
- ä¿è¯ç³»ç»Ÿçš„å¯æ‰©å±•æ€§å’Œå¯ç»´æŠ¤æ€§

#### å®ç°ç­–ç•¥
1. **åˆ†é˜¶æ®µå®æ–½**ï¼šæŒ‰åŠŸèƒ½æ¨¡å—é€æ­¥å®ç°å’Œé›†æˆ
2. **é£é™©æ§åˆ¶**ï¼šå…³é”®æ¨¡å—é‡‡ç”¨å†—ä½™è®¾è®¡
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šé’ˆå¯¹å…³é”®è·¯å¾„è¿›è¡Œä¼˜åŒ–

## å®æ–½å»ºè®®

1. **å‰æœŸå‡†å¤‡**ï¼šè¯¦ç»†çš„éœ€æ±‚åˆ†æå’ŒæŠ€æœ¯éªŒè¯
2. **å¼€å‘é˜¶æ®µ**ï¼šæ¨¡å—åŒ–å¼€å‘å’Œé›†æˆæµ‹è¯•
3. **éƒ¨ç½²ç»´æŠ¤**ï¼šå®Œå–„çš„éƒ¨ç½²æ–¹æ¡ˆå’Œç»´æŠ¤æœºåˆ¶"""
    
    def _generate_analysis_output(self, chunks: List[Dict], analysis_dimension: str) -> str:
        """ç”Ÿæˆæ·±åº¦åˆ†æè¾“å‡º"""
        return f"""åŸºäºæä¾›çš„æŠ€æœ¯èµ„æ–™ï¼Œå¯¹{analysis_dimension}è¿›è¡Œæ·±å…¥åˆ†æï¼š

## æŠ€æœ¯åŸç†åˆ†æ

### æ ¸å¿ƒæŠ€æœ¯æœºåˆ¶
æŠ€æœ¯å®ç°åŸºäºä»¥ä¸‹æ ¸å¿ƒåŸç†å’Œæœºåˆ¶ï¼š

1. **åŸºç¡€åŸç†**ï¼šé˜è¿°underlyingæŠ€æœ¯åŸç†
2. **å®ç°æœºåˆ¶**ï¼šè¯¦ç»†åˆ†æå…·ä½“å®ç°æ–¹æ³•
3. **å…³é”®æŠ€æœ¯**ï¼šè¯†åˆ«å’Œåˆ†æå…³é”®æŠ€æœ¯è¦ç‚¹

### æŠ€æœ¯ç‰¹ç‚¹åˆ†æ

#### è®¾è®¡ç†å¿µ
- æŠ€æœ¯æ–¹æ¡ˆä½“ç°äº†ç‰¹å®šçš„è®¾è®¡ç†å¿µå’Œæ€è·¯
- åœ¨æ€§èƒ½ã€åŠŸè€—ã€æˆæœ¬ç­‰æ–¹é¢è¿›è¡Œäº†å¹³è¡¡è€ƒè™‘
- å……åˆ†è€ƒè™‘äº†å®é™…åº”ç”¨åœºæ™¯çš„éœ€æ±‚

#### å®ç°ç»†èŠ‚
1. **ç®—æ³•å®ç°**ï¼šæ ¸å¿ƒç®—æ³•çš„å…·ä½“å®ç°æ–¹å¼
2. **ç¡¬ä»¶æ”¯æŒ**ï¼šç¡¬ä»¶å±‚é¢çš„æŠ€æœ¯æ”¯æŒ
3. **è½¯ä»¶é…åˆ**ï¼šè½¯ä»¶å±‚é¢çš„åè°ƒé…åˆ

## æŠ€æœ¯æ´å¯Ÿä¸å¯ç¤º

### æŠ€æœ¯ä¼˜åŠ¿
- åœ¨ç‰¹å®šåº”ç”¨é¢†åŸŸå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿
- æŠ€æœ¯å®ç°å…·æœ‰åˆ›æ–°æ€§å’Œå®ç”¨æ€§
- ä¸ºç›¸å…³æŠ€æœ¯å‘å±•æä¾›äº†æœ‰ç›Šå‚è€ƒ

### åº”ç”¨ä»·å€¼
1. **ç›´æ¥åº”ç”¨**ï¼šå¯ç›´æ¥åº”ç”¨äºç›¸å…³äº§å“å’Œç³»ç»Ÿ
2. **æŠ€æœ¯å€Ÿé‰´**ï¼šä¸ºç±»ä¼¼æŠ€æœ¯æ–¹æ¡ˆæä¾›å‚è€ƒ
3. **å‘å±•æ½œåŠ›**ï¼šå…·æœ‰è¿›ä¸€æ­¥å‘å±•å’Œä¼˜åŒ–çš„æ½œåŠ›"""
    
    def _generate_reasoning_output(self, chunks: List[Dict], problem_category: str) -> str:
        """ç”Ÿæˆæ¨ç†è§£å†³è¾“å‡º"""
        return f"""åŸºäºæä¾›çš„æŠ€æœ¯èµ„æ–™ï¼Œå¯¹{problem_category}è¿›è¡Œåˆ†æå’Œè§£å†³ï¼š

## é—®é¢˜åˆ†æ

### æ ¹å› åˆ†æ
é€šè¿‡å¯¹æŠ€æœ¯èµ„æ–™çš„ç»¼åˆåˆ†æï¼Œé—®é¢˜å¯èƒ½çš„æ ¹æœ¬åŸå› åŒ…æ‹¬ï¼š

1. **æŠ€æœ¯å±‚é¢**ï¼šæŠ€æœ¯å®ç°æˆ–é…ç½®æ–¹é¢çš„é—®é¢˜
2. **ç³»ç»Ÿå±‚é¢**ï¼šç³»ç»Ÿé›†æˆæˆ–å…¼å®¹æ€§é—®é¢˜  
3. **ç¯å¢ƒå±‚é¢**ï¼šå¤–éƒ¨ç¯å¢ƒæˆ–ä½¿ç”¨æ¡ä»¶å½±å“

### å½±å“è¯„ä¼°
- é—®é¢˜å¯¹ç³»ç»Ÿæ€§èƒ½å’Œç¨³å®šæ€§çš„å½±å“ç¨‹åº¦
- å¯èƒ½å¯¼è‡´çš„è¿é”ååº”å’Œæ¬¡ç”Ÿé—®é¢˜
- è§£å†³é—®é¢˜çš„ç´§è¿«æ€§å’Œä¼˜å…ˆçº§

## è§£å†³æ–¹æ¡ˆ

### æŠ€æœ¯è§£å†³è·¯å¾„

#### çŸ­æœŸè§£å†³æ–¹æ¡ˆ
1. **åº”æ€¥æªæ–½**ï¼šå¿«é€Ÿç¼“è§£é—®é¢˜å½±å“çš„ä¸´æ—¶æ–¹æ¡ˆ
2. **å‚æ•°è°ƒæ•´**ï¼šé€šè¿‡å‚æ•°ä¼˜åŒ–æ”¹å–„é—®é¢˜çŠ¶å†µ
3. **é…ç½®ä¿®æ­£**ï¼šä¿®æ­£å¯èƒ½çš„é…ç½®é”™è¯¯

#### é•¿æœŸä¼˜åŒ–ç­–ç•¥
1. **æ ¹æœ¬æ€§æ”¹è¿›**ï¼šä»æºå¤´è§£å†³é—®é¢˜çš„æ ¹æœ¬æ–¹æ¡ˆ
2. **ç³»ç»Ÿä¼˜åŒ–**ï¼šæ•´ä½“ç³»ç»Ÿçš„ä¼˜åŒ–å’Œå®Œå–„
3. **é¢„é˜²æœºåˆ¶**ï¼šå»ºç«‹é—®é¢˜é¢„é˜²å’Œæ—©æœŸå‘ç°æœºåˆ¶

## å®æ–½å»ºè®®

### è§£å†³æ­¥éª¤
1. **é—®é¢˜ç¡®è®¤**ï¼šå‡†ç¡®å®šä½å’Œç¡®è®¤é—®é¢˜èŒƒå›´
2. **æ–¹æ¡ˆå®æ–½**ï¼šæŒ‰è®¡åˆ’å®æ–½è§£å†³æ–¹æ¡ˆ
3. **æ•ˆæœéªŒè¯**ï¼šéªŒè¯è§£å†³æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§
4. **æŒç»­ç›‘æ§**ï¼šå»ºç«‹é•¿æœŸç›‘æ§å’Œç»´æŠ¤æœºåˆ¶

### é£é™©æ§åˆ¶
- åœ¨å®æ–½è¿‡ç¨‹ä¸­æ³¨æ„æ§åˆ¶é£é™©
- å»ºç«‹å›é€€æ–¹æ¡ˆä»¥åº”å¯¹æ„å¤–æƒ…å†µ
- ç¡®ä¿è§£å†³æ–¹æ¡ˆä¸ä¼šå¼•å…¥æ–°çš„é—®é¢˜"""
    
    def _determine_difficulty(self, chunks: List[Dict]) -> str:
        """ç¡®å®šä»»åŠ¡éš¾åº¦"""
        difficulty_dist = self.config["dataset_config"]["difficulty_distribution"]
        
        # ç®€å•çš„å¯å‘å¼è§„åˆ™
        total_length = sum(len(chunk.get("text", "")) for chunk in chunks)
        unique_sources = len(set(chunk.get("metadata", {}).get("chip_category", "") for chunk in chunks))
        
        if total_length < 1000 or unique_sources <= 2:
            return "basic"
        elif total_length < 2000 or unique_sources <= 3:
            return "intermediate"
        else:
            return "advanced"
    
    def _calculate_quality_score(self, instruction: str, context: str, output: str) -> float:
        """è®¡ç®—è´¨é‡è¯„åˆ†"""
        quality_criteria = self.config.get("quality_criteria", {})
        
        score = 0.0
        
        # ä¸Šä¸‹æ–‡è¿è´¯æ€§ (ç®€åŒ–è¯„ä¼°)
        context_coherence = min(1.0, len(context) / 1000)  # åŸºäºé•¿åº¦çš„ç®€å•è¯„ä¼°
        score += context_coherence * quality_criteria.get("context_coherence", {}).get("weight", 0.25)
        
        # æ•´åˆæ·±åº¦ (åŸºäºè¾“å‡ºé•¿åº¦å’Œç»“æ„)
        integration_depth = min(1.0, len(output) / 500)
        score += integration_depth * quality_criteria.get("integration_depth", {}).get("weight", 0.30)
        
        # æ¨ç†é€»è¾‘ (åŸºäºæŒ‡ä»¤å’Œè¾“å‡ºçš„åŒ¹é…åº¦)
        reasoning_logic = 0.8  # å›ºå®šå€¼ï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥ç”¨æ›´å¤æ‚çš„æ–¹æ³•
        score += reasoning_logic * quality_criteria.get("reasoning_logic", {}).get("weight", 0.25)
        
        # å®ç”¨ä»·å€¼ (åŸºäºå†…å®¹ä¸°å¯Œåº¦)
        practical_value = min(1.0, (len(instruction) + len(output)) / 1000)
        score += practical_value * quality_criteria.get("practical_value", {}).get("weight", 0.20)
        
        return round(score, 3)
    
    def _validate_task_quality(self, task: IntegrationTask) -> bool:
        """éªŒè¯ä»»åŠ¡è´¨é‡"""
        # åŸºæœ¬è´¨é‡æ£€æŸ¥
        if len(task.instruction) < 10:
            return False
        if len(task.context) < 100:
            return False
        if len(task.expected_output) < 50:
            return False
        if task.quality_score < 0.3:
            return False
        
        return True
    
    def save_dataset(self, dataset: List[IntegrationTask]):
        """ä¿å­˜æ•°æ®é›†ï¼Œæ”¯æŒAlpacaå’ŒShareGPTæ ¼å¼"""
        output_config = self.config["output_format"]
        
        # ä¿å­˜Alpacaæ ¼å¼
        if output_config.get("alpaca_format", True):
            self._save_alpaca_format(dataset)
        
        # ä¿å­˜ShareGPTæ ¼å¼
        if output_config.get("sharegpt_format", False):
            self._save_sharegpt_format(dataset)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        self._save_statistics(dataset)
    
    def _save_alpaca_format(self, dataset: List[IntegrationTask]):
        """ä¿å­˜ä¸ºAlpacaæ ¼å¼"""
        output_config = self.config["output_format"]
        include_metadata = output_config.get("include_metadata", True)
        metadata_fields = output_config.get("metadata_fields", [])
        
        alpaca_data = []
        for task in dataset:
            # æ ‡å‡†Alpacaå­—æ®µ
            alpaca_entry = {
                "instruction": task.instruction,
                "input": task.context,  # Alpacaæ ¼å¼ä¸­inputå­—æ®µç”¨äºä¸Šä¸‹æ–‡
                "output": task.expected_output
            }
            
            # æ·»åŠ å…ƒæ•°æ®å­—æ®µ
            if include_metadata:
                for field in metadata_fields:
                    if hasattr(task, field):
                        alpaca_entry[field] = getattr(task, field)
            
            alpaca_data.append(alpaca_entry)
        
        # ä¿å­˜Alpacaæ ¼å¼æ–‡ä»¶
        alpaca_path = self.output_dir / "alpaca_integration_dataset.json"
        with open(alpaca_path, 'w', encoding='utf-8') as f:
            json.dump(alpaca_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Alpacaæ ¼å¼æ•°æ®é›†å·²ä¿å­˜åˆ°: {alpaca_path}")
    
    def _save_sharegpt_format(self, dataset: List[IntegrationTask]):
        """ä¿å­˜ä¸ºShareGPTæ ¼å¼"""
        output_config = self.config["output_format"]
        include_metadata = output_config.get("include_metadata", True)
        metadata_fields = output_config.get("metadata_fields", [])
        
        sharegpt_data = []
        for task in dataset:
            # ShareGPTå¯¹è¯æ ¼å¼
            sharegpt_entry = {
                "conversations": [
                    {
                        "from": "human", 
                        "value": f"{task.instruction}\n\n{task.context}"
                    },
                    {
                        "from": "gpt",
                        "value": task.expected_output
                    }
                ]
            }
            
            # æ·»åŠ å…ƒæ•°æ®å­—æ®µ
            if include_metadata:
                for field in metadata_fields:
                    if hasattr(task, field):
                        sharegpt_entry[field] = getattr(task, field)
            
            sharegpt_data.append(sharegpt_entry)
        
        # ä¿å­˜ShareGPTæ ¼å¼æ–‡ä»¶
        sharegpt_path = self.output_dir / "sharegpt_integration_dataset.json"
        with open(sharegpt_path, 'w', encoding='utf-8') as f:
            json.dump(sharegpt_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ShareGPTæ ¼å¼æ•°æ®é›†å·²ä¿å­˜åˆ°: {sharegpt_path}")
    
    def _save_statistics(self, dataset: List[IntegrationTask]):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_samples": len(dataset),
            "format_info": {
                "alpaca_compatible": True,
                "sharegpt_compatible": True,
                "format_compliance": "å®Œå…¨ç¬¦åˆAlpacaå’ŒShareGPTæ ‡å‡†æ ¼å¼"
            },
            "task_type_distribution": {},
            "difficulty_distribution": {},
            "quality_scores": {
                "avg_quality_score": sum(task.quality_score for task in dataset) / len(dataset),
                "min_quality_score": min(task.quality_score for task in dataset),
                "max_quality_score": max(task.quality_score for task in dataset)
            },
            "length_statistics": {
                "avg_instruction_length": sum(len(task.instruction) for task in dataset) / len(dataset),
                "avg_context_length": sum(len(task.context) for task in dataset) / len(dataset),
                "avg_output_length": sum(len(task.expected_output) for task in dataset) / len(dataset),
                "max_context_length": max(len(task.context) for task in dataset)
            },
            "integration_analysis": {
                "unique_context_sources": len(set(source for task in dataset for source in task.context_sources)),
                "avg_sources_per_task": sum(len(task.context_sources) for task in dataset) / len(dataset)
            },
            "generated_at": datetime.now().isoformat(),
            "config_used": self.config
        }
        
        # ç»Ÿè®¡ä»»åŠ¡ç±»å‹åˆ†å¸ƒ
        for task in dataset:
            task_type = task.task_type
            stats["task_type_distribution"][task_type] = stats["task_type_distribution"].get(task_type, 0) + 1
        
        # ç»Ÿè®¡éš¾åº¦åˆ†å¸ƒ
        for task in dataset:
            difficulty = task.difficulty
            stats["difficulty_distribution"][difficulty] = stats["difficulty_distribution"].get(difficulty, 0) + 1
        
        # ä¿å­˜ç»Ÿè®¡æ–‡ä»¶
        stats_path = self.output_dir / "dataset_statistics.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_path}")

def main():
    """ä¸»å‡½æ•°"""
    builder = ConfigurableIntegrationDatasetBuilder(
        config_file="integration_config.json"
    )
    
    # æ„å»ºæ•°æ®é›†
    dataset = builder.build_integration_dataset()
    
    if not dataset:
        logger.error("æœªèƒ½ç”Ÿæˆä»»ä½•æ•°æ®é›†æ ·æœ¬ï¼")
        return
    
    # ä¿å­˜æ•°æ®é›†ï¼ˆæ”¯æŒAlpacaå’ŒShareGPTæ ¼å¼ï¼‰
    builder.save_dataset(dataset)
    
    logger.info("=" * 60)
    logger.info("ğŸ‰ æ•°æ®é›†æ„å»ºå®Œæˆï¼")
    logger.info("=" * 60)
    logger.info(f"æ€»æ ·æœ¬æ•°: {len(dataset)}")
    logger.info(f"è¾“å‡ºç›®å½•: {builder.output_dir}")
    logger.info("")
    logger.info("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    logger.info("  - alpaca_integration_dataset.json (Alpacaæ ¼å¼)")
    if builder.config["output_format"].get("sharegpt_format"):
        logger.info("  - sharegpt_integration_dataset.json (ShareGPTæ ¼å¼)")
    logger.info("  - dataset_statistics.json (è¯¦ç»†ç»Ÿè®¡)")
    logger.info("")
    logger.info("âœ… æ ¼å¼å…¼å®¹æ€§:")
    logger.info("  - Alpacaæ ¼å¼: å®Œå…¨å…¼å®¹")
    logger.info("  - ShareGPTæ ¼å¼: å®Œå…¨å…¼å®¹")
    logger.info("  - åŒ…å«ä¸°å¯Œçš„å…ƒæ•°æ®å’Œè´¨é‡è¯„åˆ†")
    
    # æ˜¾ç¤ºç¤ºä¾‹
    if dataset:
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“‹ æ•°æ®é›†ç¤ºä¾‹é¢„è§ˆ:")
        logger.info("=" * 60)
        
        sample_task = dataset[0]
        logger.info(f"ä»»åŠ¡ç±»å‹: {sample_task.task_type}")
        logger.info(f"éš¾åº¦çº§åˆ«: {sample_task.difficulty}")
        logger.info(f"è´¨é‡è¯„åˆ†: {sample_task.quality_score}")
        logger.info(f"æŒ‡ä»¤: {sample_task.instruction[:100]}...")
        logger.info(f"ä¸Šä¸‹æ–‡é•¿åº¦: {len(sample_task.context)} å­—ç¬¦")
        logger.info(f"è¾“å‡ºé•¿åº¦: {len(sample_task.expected_output)} å­—ç¬¦")
        logger.info(f"ä¸Šä¸‹æ–‡æ¥æº: {', '.join(sample_task.context_sources[:3])}")

if __name__ == "__main__":
    main()
