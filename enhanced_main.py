#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºå‘é‡åŒ–å¤„ç†è„šæœ¬
ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡ä¸­æ–‡è¯­ä¹‰embeddingå¹¶è¿›è¡Œè¯­ä¹‰æœç´¢
"""

import sys
import json
import logging
from pathlib import Path
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_vectorization.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åŒ…"""
    logger.info("æ£€æŸ¥ä¾èµ–åŒ…...")
    
    dependencies = {
        'numpy': 'numpy',
        'pandas': 'pandas', 
        'scikit-learn': 'sklearn',
        'jieba': 'jieba',
        'sentence-transformers': 'sentence_transformers',
        'transformers': 'transformers',
        'torch': 'torch'
    }
    
    missing = []
    available = []
    
    for name, import_name in dependencies.items():
        try:
            __import__(import_name)
            available.append(name)
        except ImportError:
            missing.append(name)
    
    logger.info(f"å¯ç”¨åŒ…: {', '.join(available)}")
    if missing:
        logger.warning(f"ç¼ºå¤±åŒ…: {', '.join(missing)}")
        logger.info("è¯·è¿è¡Œ: uv add " + " ".join(missing))
    
    return len(missing) == 0

def install_required_packages():
    """å®‰è£…å¿…éœ€çš„åŒ…"""
    logger.info("å®‰è£…å¿…éœ€çš„åŒ…...")
    
    import subprocess
    
    packages = [
        "sentence-transformers",
        "transformers", 
        "torch"
    ]
    
    for package in packages:
        try:
            logger.info(f"å®‰è£… {package}...")
            result = subprocess.run([
                sys.executable, "-m", "pip", "install", package
            ], capture_output=True, text=True, encoding='utf-8')
            
            if result.returncode == 0:
                logger.info(f"{package} å®‰è£…æˆåŠŸ")
            else:
                logger.error(f"{package} å®‰è£…å¤±è´¥: {result.stderr}")
        except Exception as e:
            logger.error(f"å®‰è£… {package} æ—¶å‡ºé”™: {e}")

def run_enhanced_vectorization(input_file: str, output_dir: str, model_type: str = "sentence_transformers"):
    """è¿è¡Œå¢å¼ºå‘é‡åŒ–"""
    logger.info("=" * 60)
    logger.info("å¼€å§‹å¢å¼ºå‘é‡åŒ–å¤„ç†")
    logger.info("=" * 60)
    
    try:
        from enhanced_vectorizer import VectorizationPipeline, EmbeddingConfig
        
        # åˆ›å»ºé…ç½®
        config = EmbeddingConfig(
            model_type=model_type,
            device="cpu",  # é»˜è®¤ä½¿ç”¨CPUï¼Œå¦‚æœéœ€è¦GPUå¯ä»¥æ”¹ä¸º"cuda"
            batch_size=16  # å‡å°æ‰¹å¤„ç†å¤§å°ä»¥é€‚åº”CPU
        )
        
        if model_type == "sentence_transformers":
            # ä½¿ç”¨å¤šè¯­è¨€æ¨¡å‹
            config.model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        elif model_type == "transformers":
            # ä½¿ç”¨ä¸­æ–‡BERTæ¨¡å‹
            config.model_name = "bert-base-chinese"
        
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {config.model_name}")
        logger.info(f"æ¨¡å‹ç±»å‹: {config.model_type}")
        logger.info(f"è®¡ç®—è®¾å¤‡: {config.device}")
        
        # åˆ›å»ºå¤„ç†æµæ°´çº¿
        pipeline = VectorizationPipeline(config)
        
        # å¤„ç†æ•°æ®
        stats = pipeline.process_vectorization_data(input_file, output_dir)
        
        logger.info("=" * 60)
        logger.info("å¢å¼ºå‘é‡åŒ–å¤„ç†å®Œæˆ")
        logger.info("=" * 60)
        logger.info(f"å¤„ç†ç»Ÿè®¡:")
        logger.info(f"  - æ€»æ•°æ®å—: {stats['total_chunks']}")
        logger.info(f"  - Embeddingç»´åº¦: {stats['embedding_dimension']}")
        logger.info(f"  - å¤„ç†æ—¶é—´: {stats['processing_time']:.2f}ç§’")
        logger.info(f"  - è¾“å‡ºç›®å½•: {output_dir}")
        
        return stats
        
    except ImportError as e:
        logger.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        logger.info("è¯·ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…")
        return None
    except Exception as e:
        logger.error(f"å‘é‡åŒ–å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def run_semantic_search_demo(embedding_dir: str):
    """è¿è¡Œè¯­ä¹‰æœç´¢æ¼”ç¤º"""
    logger.info("=" * 60) 
    logger.info("è¯­ä¹‰æœç´¢æ¼”ç¤º")
    logger.info("=" * 60)
    
    try:
        from semantic_search import AdvancedSearchInterface
        
        # åˆ›å»ºæœç´¢æ¥å£
        search_interface = AdvancedSearchInterface(embedding_dir)
        
        # é¢„è®¾æŸ¥è¯¢ç¤ºä¾‹
        demo_queries = [
            "MCUä½åŠŸè€—æ¨¡å¼",
            "GPIOå¼•è„šé…ç½®",
            "å®šæ—¶å™¨ä¸­æ–­",
            "SPIé€šä¿¡åè®®",
            "ADCæ¨¡æ•°è½¬æ¢",
            "è§¦æ‘¸æ§åˆ¶èŠ¯ç‰‡",
            "ç”µæœºé©±åŠ¨åº”ç”¨"
        ]
        
        logger.info("è¿è¡Œæ¼”ç¤ºæŸ¥è¯¢...")
        
        for query in demo_queries:
            logger.info(f"\næœç´¢: '{query}'")
            try:
                results = search_interface.multi_modal_search(query, top_k=3)
                
                if results:
                    for i, result in enumerate(results, 1):
                        logger.info(f"  {i}. ç›¸ä¼¼åº¦: {result.score:.3f} [{result.chunk_type}]")
                        logger.info(f"     æ–‡æœ¬: {result.text[:80]}...")
                else:
                    logger.info("     æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
                    
            except Exception as e:
                logger.error(f"æŸ¥è¯¢ '{query}' å¤±è´¥: {e}")
        
        logger.info("\n" + "=" * 60)
        logger.info("è¯­ä¹‰æœç´¢æ¼”ç¤ºå®Œæˆ")
        logger.info("=" * 60)
        
    except ImportError as e:
        logger.error(f"å¯¼å…¥æœç´¢æ¨¡å—å¤±è´¥: {e}")
    except Exception as e:
        logger.error(f"æœç´¢æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def interactive_search(embedding_dir: str):
    """äº¤äº’å¼æœç´¢"""
    logger.info("=" * 60)
    logger.info("äº¤äº’å¼è¯­ä¹‰æœç´¢")
    logger.info("=" * 60)
    
    try:
        from semantic_search import AdvancedSearchInterface
        
        search_interface = AdvancedSearchInterface(embedding_dir)
        
        print("\næ¬¢è¿ä½¿ç”¨è¯­ä¹‰æœç´¢ç³»ç»Ÿ!")
        print("è¾“å…¥æŸ¥è¯¢å†…å®¹ï¼Œè¾“å…¥ 'quit' é€€å‡º")
        print("-" * 40)
        
        while True:
            try:
                query = input("\nè¯·è¾“å…¥æœç´¢æŸ¥è¯¢: ").strip()
                
                if query.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                    break
                
                if not query:
                    continue
                
                print(f"\næœç´¢: '{query}'")
                print("-" * 40)
                
                results = search_interface.multi_modal_search(query, top_k=5)
                
                if results:
                    for i, result in enumerate(results, 1):
                        print(f"\n{i}. [{result.chunk_type}] ç›¸ä¼¼åº¦: {result.score:.3f}")
                        print(f"   ID: {result.chunk_id}")
                        print(f"   æ–‡æœ¬: {result.text[:150]}...")
                        
                        # æ˜¾ç¤ºå…ƒæ•°æ®
                        metadata = result.metadata
                        if metadata:
                            meta_info = []
                            if 'chip_category' in metadata:
                                meta_info.append(f"ç±»åˆ«: {metadata['chip_category']}")
                            if 'document_type' in metadata:
                                meta_info.append(f"ç±»å‹: {metadata['document_type']}")
                            if 'file_name' in metadata:
                                meta_info.append(f"æ–‡ä»¶: {metadata['file_name']}")
                            
                            if meta_info:
                                print(f"   {' | '.join(meta_info)}")
                else:
                    print("   æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
                
            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"   æœç´¢å‡ºé”™: {e}")
        
        print("\næ„Ÿè°¢ä½¿ç”¨!")
        
    except ImportError as e:
        logger.error(f"å¯¼å…¥æœç´¢æ¨¡å—å¤±è´¥: {e}")
    except Exception as e:
        logger.error(f"äº¤äº’å¼æœç´¢å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å¢å¼ºå‘é‡åŒ–å’Œè¯­ä¹‰æœç´¢ç³»ç»Ÿ')
    parser.add_argument('--action', choices=['vectorize', 'search', 'demo', 'interactive', 'install'], 
                       default='vectorize', help='æ“ä½œç±»å‹')
    parser.add_argument('--input', help='è¾“å…¥çš„å‘é‡åŒ–å‡†å¤‡æ•°æ®æ–‡ä»¶')
    parser.add_argument('--output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--embedding-dir', help='embeddingæ•°æ®ç›®å½•ï¼ˆç”¨äºæœç´¢ï¼‰')
    parser.add_argument('--model-type', choices=['sentence_transformers', 'transformers', 'tfidf'],
                       default='sentence_transformers', help='æ¨¡å‹ç±»å‹')
    parser.add_argument('--query', help='æœç´¢æŸ¥è¯¢')
    
    args = parser.parse_args()
    
    # é»˜è®¤è·¯å¾„
    default_input = "./output/vectorization/vectorization_ready.json"
    default_output = "./output/enhanced_embeddings"
    
    if args.action == 'install':
        install_required_packages()
        return
    
    if args.action == 'vectorize':
        input_file = args.input or default_input
        output_dir = args.output or default_output
        
        if not Path(input_file).exists():
            logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            logger.info("è¯·å…ˆè¿è¡Œä¸»å¤„ç†æµç¨‹ç”Ÿæˆå‘é‡åŒ–å‡†å¤‡æ•°æ®")
            return
        
        # æ£€æŸ¥ä¾èµ–
        if not check_dependencies():
            logger.info("æ­£åœ¨å®‰è£…ç¼ºå¤±çš„ä¾èµ–åŒ…...")
            install_required_packages()
        
        # è¿è¡Œå‘é‡åŒ–
        stats = run_enhanced_vectorization(input_file, output_dir, args.model_type)
        
        if stats:
            print(f"\nâœ… å¢å¼ºå‘é‡åŒ–å®Œæˆ!")
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   - æ€»æ•°æ®å—: {stats['total_chunks']}")
            print(f"   - Embeddingç»´åº¦: {stats['embedding_dimension']}")
            print(f"   - å¤„ç†æ—¶é—´: {stats['processing_time']:.2f}ç§’")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
            print(f"\nğŸ’¡ æ¥ä¸‹æ¥å¯ä»¥ä½¿ç”¨æœç´¢åŠŸèƒ½:")
            print(f"   python enhanced_main.py --action demo --embedding-dir {output_dir}")
    
    elif args.action == 'search':
        embedding_dir = args.embedding_dir or default_output
        
        if not Path(embedding_dir).exists():
            logger.error(f"embeddingç›®å½•ä¸å­˜åœ¨: {embedding_dir}")
            logger.info("è¯·å…ˆè¿è¡Œå‘é‡åŒ–å¤„ç†")
            return
        
        if args.query:
            # å•æ¬¡æœç´¢
            from semantic_search import AdvancedSearchInterface
            search_interface = AdvancedSearchInterface(embedding_dir)
            results = search_interface.multi_modal_search(args.query, top_k=5)
            
            print(f"\næœç´¢ç»“æœ for '{args.query}':")
            for i, result in enumerate(results, 1):
                print(f"{i}. [{result.chunk_type}] ç›¸ä¼¼åº¦: {result.score:.3f}")
                print(f"   {result.text[:100]}...")
        else:
            logger.error("è¯·æä¾›æœç´¢æŸ¥è¯¢ (--query)")
    
    elif args.action == 'demo':
        embedding_dir = args.embedding_dir or default_output
        
        if not Path(embedding_dir).exists():
            logger.error(f"embeddingç›®å½•ä¸å­˜åœ¨: {embedding_dir}")
            return
        
        run_semantic_search_demo(embedding_dir)
    
    elif args.action == 'interactive':
        embedding_dir = args.embedding_dir or default_output
        
        if not Path(embedding_dir).exists():
            logger.error(f"embeddingç›®å½•ä¸å­˜åœ¨: {embedding_dir}")
            return
        
        interactive_search(embedding_dir)

if __name__ == "__main__":
    main()
